---
# This configuration file has two sections.  The first is the Vagrant boxes 
# subdivided into ceph installation methods with related repositories and 
# packages.  The second is the various server configurations to simulate 
# different environments.
#
########################################
# Installation Section
########################################
#
# The vagrant box is a JeOS image and has no initial repositories.  Create new
# entries to use your own repositories or add repositories to the existing lists.
#
# Change the INSTALLATION to either 'ceph-deploy' or 'vsm' in the Vagrantfile.
########################################
# OpenSUSE 13.2
########################################
openSUSE-13.2:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/distribution/13.2/repo/oss/'
      #'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_13.2/'
      'Ceph': 'http://download.suse.de/ibs/Devel:/Storage:/1.0/openSUSE_Factory/'
      'swiftgist/openSUSE 13.2': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_13.2/'
      'Devel C Libraries': 
        'http://download.opensuse.org/repositories/devel:libraries:c_c++/openSUSE_13.2/'
    packages:
      all:
        - ceph-deploy
        - vim
        - vim-data
      admin:
        - ceph-deploy
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/distribution/13.2/repo/oss/'
      'swiftgist/openSUSE 13.2': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_13.2/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_13.2/'
      'Openstack - Juno': 'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/openSUSE_13.2/'
    packages:
      all:
        - vim
        - vim-data
        - lsb-release
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - curl
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
      data1: true
      data2: true
      data3: true
    commands:
      admin:
        - vsm-controller
        - ssh data1 vsm-storage
        - ssh data2 vsm-storage
        - ssh data3 vsm-storage
########################################
# OpenSUSE Tumbleweed
########################################
Tumbleweed:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/tumbleweed/repo/oss/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Tumbleweed/'
    packages:
      all:
        - vim
        - vim-data
      admin:
        - ceph-deploy
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/tumbleweed/repo/oss/'
      # 'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Tumbleweed/'
      'Ceph IBS': 'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/openSUSE_Tumbleweed/'
      'swiftgist/openSUSE Tumbleweed': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_Tumbleweed/'
      'Openstack - Juno': 'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/openSUSE_Factory/'
    packages:
      all:
        - vim
        - vim-data
        - lsb-release
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - curl
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
      data1: true
      data2: true
      data3: true
    commands:
      admin:
        - vsm-controller
        - ssh data1 vsm-storage
        - ssh data2 vsm-storage
        - ssh data3 vsm-storage
########################################
# SLE 12
########################################
SLE-12:
  ceph-deploy:
    repos:
      'SLE12 Pool (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'SLE12 Pool Updates (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/Update/standard/'
      'SDK':
        'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Products/SLE-SDK/12/x86_64/product'
      'SDK Updates':
        'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Updates/SLE-SDK/12/x86_64/update'
      #'Ceph': 
      #  'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      #'Ceph': 
      #  'http://download.suse.de/ibs/Devel:/Storage:/1.0:/Staging/SLE_12/'
      'Ceph':
        'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/SLE12/'
      'Python Devel': 
        'http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12/'
    packages:
      all:
        - vim
        - vim-data
      admin:
        - ceph-deploy
    commands:
      admin:
        - echo Done
  salt:
    repos:
      #'SLE12 Pool (OSS)': 
      #  'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      #'SLE12 Pool Updates (OSS)': 
      #  'http://download.suse.de/ibs/SUSE:/SLE-12:/Update/standard/'
      #'SDK':
      #  'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Products/SLE-SDK/12/x86_64/product'
      #'SDK Updates':
      #  'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Updates/SLE-SDK/12/x86_64/update'
      ##'Ceph': 
      ##  'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      ##'Ceph': 
      ##  'http://download.suse.de/ibs/Devel:/Storage:/1.0:/Staging/SLE_12/'
      #'Ceph':
      #  'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/SLE12/'
      #'Python Devel': 
      #  'http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12/'
      #'SES2 Staging': 
      #  'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/SLE12/'
      'sle-sdk': 'http://192.168.10.95/repo/products/sle-sdk/product'
      'SLE 12': 'http://192.168.10.95/repo/products/SLE_12/product'
      'Storage': 'http://192.168.10.95/repo/products/storage/product'
      'sle-sdk update': 'http://192.168.10.95/repo/updates/sle-sdk/update'
      'SLE 12 update': 'http://192.168.10.95/repo/updates/SLE_12/update'
      'Storage update': 'http://192.168.10.95/repo/updates/storage/update'
      'Goldwyn': 'http://download.suse.de/ibs/home:/goldwynr:/Kernel:/SLE12-SP1/standard/'
    packages:
      all:
        - vim
        - vim-data
        - salt-minion
        - diamond
      admin:
        - ceph-deploy
        - git
      calamari:
        - salt-master
        - postgresql93-9.3.8-8.1
        - calamari-server
    files:
      all: true
      admin: true
    commands:
      admin:
        - rpm -e salt-minion salt || exit 0
        - zypper -n ar 'http://192.168.10.95/repo/systemsmanagement:/saltstack/SLE_12' 'System Management' || exit 0
        - sleep 3
        - zypper --gpg-auto-import-keys -n in salt-master-2015.8.3
        - /srv/reactor/ready_check --init
        - systemctl start salt-master
        - systemctl enable salt-master
        - start_minions 
        - sleep 10
        - salt-key -y -A  || exit 0
        - git clone https://github.com/saltstack/salt.git || exit 0
          #- salt-run state.orchestrate orch.ceph
          #- start_calamari_minions
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'Docker Devel': 'http://download.suse.de/ibs/Devel:/Docker/SLE_12/'
      'Python Devel': 
        'http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12/'
      'Ceph': 
        'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      'swiftgist/SLE_12': 
        'http://download.opensuse.org/repositories/home:/swiftgist/SLE_12/'
      'Openstack - Juno': 
        'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/SLE_12/'
    packages:
      all:
        - vim
        - vim-data
        - lsb-release
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - curl
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
      data1: true
      data2: true
      data3: true
    commands:
      admin:
        - vsm-controller
        - ssh data1 vsm-storage
        - ssh data2 vsm-storage
        - ssh data3 vsm-storage

########################################
# OpenSUSE Leap 42.1
########################################
openSUSE-42.1:
  openattic+:
    repos:
      'OpenAttic': 'http://download.opensuse.org/repositories/filesystems:/openATTIC/openSUSE_Leap_42.1/'
      'Ceph Jewel': 'http://download.opensuse.org/repositories/filesystems:/ceph:/jewel/openSUSE_Leap_42.1/'
    packages:
      admin:
        - pnp4nagios-0.6.25-8.1
        - openattic
        - openattic-gui
        - ceph-deploy
        - git
        - salt-master
      all:
        - vim
        - vim-data
        - man
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: true
      admin: true
    commands:
      admin:
        - systemctl enable apache2
        - systemctl start apache2
        - oaconfig install --allow-broken-hostname
        - zypper -n in -t pattern Basis-Devel
        - zypper -n in python-devel libgit2-devel
        - /srv/reactor/ready_check --init
        - systemctl start salt-master
        - systemctl enable salt-master
        - /usr/local/sbin/start_minions
        - sleep 10
        - salt-key -y -A  || exit 0
        - git clone https://github.com/saltstack/salt.git || exit 0
  openattic:
    repos:
      'OpenAttic': 'http://download.opensuse.org/repositories/filesystems:/openATTIC/openSUSE_Leap_42.1/'
    packages:
      admin:
        #- monitoring-plugins-http
        #- monitoring-plugins-swap
        #- monitoring-plugins-ssh
        #- monitoring-plugins-ping
        #- monitoring-plugins-disk
        - pnp4nagios-0.6.25-8.1
        - openattic
        - openattic-gui
      all:
        - vim
        - vim-data
        - man
    commands:
      admin:
        - ln -s /etc/icinga /etc/nagios3
        - ln -s /etc/icinga/icinga.cfg /etc/icinga/nagios.cfg
        - ln -s /usr/sbin/icinga /usr/sbin/nagios
        - systemctl enable apache2
        - systemctl start apache2
        - oaconfig install --allow-broken-hostname
  salt:
    repos:
      'sle-sdk': 'http://192.168.10.95/repo/products/sle-sdk-12-SP1/product'
      'sle-sdk update': 'http://192.168.10.95/repo/updates/sle-sdk-12-SP1/update'
      'SLE 12 SP1': 'http://192.168.10.95/repo/products/SLE_12-SP1/product'
      'SLE 12 SP1 update': 'http://192.168.10.95/repo/updates/SLE_12-SP1/update'
      'SES-image': 'http://192.168.10.95/repo/products/ses-2.1-image/SUSE-Enterprise-Storage-2.1-POOL-x86_64-Build0012-Media1'
      'SES': 'http://192.168.10.95/repo/products/ses-2.1/standard'
      'System Management': 'http://192.168.10.95/repo/systemsmanagement:/saltstack/SLE_12'
    packages:
      all:
        - vim
        - vim-data
        - salt-minion
          #- diamond
      admin:
        - ceph-deploy
        - git
        - salt-master
      calamari:
        #- salt-master
        #- postgresql93-9.3.8-8.1
        #- calamari-server
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: true
      admin: true
    commands:
      admin:
        - zypper -n in -t pattern Basis-Devel
        - zypper -n in python-devel libgit2-devel
        - /srv/reactor/ready_check --init
        - systemctl start salt-master
        - systemctl enable salt-master
        - /usr/local/sbin/start_minions
        - sleep 10
        - salt-key -y -A  || exit 0
        - git clone https://github.com/saltstack/salt.git || exit 0
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master
  two_salts:
    repos:
      'sle-sdk': 'http://192.168.10.95/repo/products/sle-sdk-12-SP1/product'
      'sle-sdk update': 'http://192.168.10.95/repo/updates/sle-sdk-12-SP1/update'
      'SLE 12 SP1': 'http://192.168.10.95/repo/products/SLE_12-SP1/product'
      'SLE 12 SP1 update': 'http://192.168.10.95/repo/updates/SLE_12-SP1/update'
      'SES-image': 'http://192.168.10.95/repo/products/ses-2.1-image/SUSE-Enterprise-Storage-2.1-POOL-x86_64-Build0012-Media1'
      'SES': 'http://192.168.10.95/repo/products/ses-2.1/standard'
      'Goldwyn': 'http://download.suse.de/ibs/home:/goldwynr:/Kernel:/SLE12-SP1/standard/'
      'Nodejs': 'http://download.opensuse.org/repositories/devel:/languages:/nodejs/SLE_12/'
    packages:
      all:
        - vim
        - vim-data
        - salt-minion
        - diamond
      admin:
        - ceph-deploy
        - git
          #- salt-master
      calamari:
        - salt-master
        - postgresql94
        - calamari-server
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: true
      admin: true
    commands:
      admin:
        - rpm -e salt-minion salt || exit 0
        - zypper -n ar 'http://192.168.10.95/repo/systemsmanagement:/saltstack/SLE_12' 'System Management' || exit 0
        - zypper -n in -t pattern Basis-Devel
        - zypper --gpg-auto-import-keys -n in python-devel libgit2-devel
        - sleep 3
        - zypper --gpg-auto-import-keys -n in salt-master-2015.8.2
        - /srv/reactor/ready_check --init
        - systemctl start salt-master
        - systemctl enable salt-master
        - /usr/local/sbin/start_minions
        - sleep 10
        - salt-key -y -A  || exit 0
        - git clone https://github.com/saltstack/salt.git || exit 0
          #- salt-run state.orchestrate orch.ceph
          #- start_calamari_minions
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master

########################################
# SLE 12 SP2 SES54
########################################
SLE12-SP2-SES5:
  openattic:
    repos:
      'OpenAttic': 'http://download.opensuse.org/repositories/filesystems:/openATTIC/SLE_12_SP2/'
      'sle-sdk': 'http://download.suse.de/ibs/SUSE/Products/SLE-SDK/12-SP2/x86_64/product/'
      'sle-sdk update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SDK/12-SP2/x86_64/update/'
      'SLE 12 update': 'http://download.suse.de/ibs/SUSE/Products/SLE-SERVER/12-SP2/x86_64/product/'
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES5/standard/'
    packages:
      admin:
        #- monitoring-plugins-http
        #- monitoring-plugins-swap
        #- monitoring-plugins-ssh
        #- monitoring-plugins-ping
        #- monitoring-plugins-disk
        - pnp4nagios-0.6.25-8.3
        - openattic
        - openattic-gui
      all:
        - vim
        - vim-data
        - man
    files:
      admin: true
    commands:
      admin:
        - ln -s /etc/icinga /etc/nagios3
        - ln -s /etc/icinga/icinga.cfg /etc/icinga/nagios.cfg
        - ln -s /usr/sbin/icinga /usr/sbin/nagios
        - systemctl enable apache2
        - systemctl start apache2
        - oaconfig install --allow-broken-hostname
  salt:
    repos:
      'sle-sdk': 'http://download.suse.de/ibs/SUSE/Products/SLE-SDK/12-SP2/x86_64/product/'
      'sle-sdk update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SDK/12-SP2/x86_64/update/'
      'SLE 12 update': 'http://download.suse.de/ibs/SUSE/Products/SLE-SERVER/12-SP2/x86_64/product/'
      'Salt': 'http://download.opensuse.org/repositories/systemsmanagement:/saltstack:/products/SLE_12_SP2/'
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES5/standard/'
      'sle12updates': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update/standard/'
    packages:
      all:
        - vim
        - vim-data
        - python-setuptools
        - salt-minion
      admin:
        - salt-master
        - ntp
        - tree
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
      all:
        - systemctl restart salt-minion
        - systemctl enable salt-minion
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master

########################################
# SLE 12 SP2 SES4
########################################
SLE12-SP2:
  openattic:
    repos:
      'OpenAttic': 'http://download.opensuse.org/repositories/filesystems:/openATTIC/SLE_12_SP2/'
      'sle-sdk': 'http://download.suse.de/ibs/SUSE/Products/SLE-SDK/12-SP2/x86_64/product/'
      'sle-sdk update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SDK/12-SP2/x86_64/update/'
      'SLE 12 update': 'http://download.suse.de/ibs/SUSE/Products/SLE-SERVER/12-SP2/x86_64/product/'
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES5/standard/'
    packages:
      admin:
        #- monitoring-plugins-http
        #- monitoring-plugins-swap
        #- monitoring-plugins-ssh
        #- monitoring-plugins-ping
        #- monitoring-plugins-disk
        - pnp4nagios-0.6.25-8.3
        - openattic
        - openattic-gui
      all:
        - vim
        - vim-data
        - man
    files:
      admin: true
    commands:
      admin:
        - ln -s /etc/icinga /etc/nagios3
        - ln -s /etc/icinga/icinga.cfg /etc/icinga/nagios.cfg
        - ln -s /usr/sbin/icinga /usr/sbin/nagios
        - systemctl enable apache2
        - systemctl start apache2
        - oaconfig install --allow-broken-hostname
  salt:
    repos:
      'sle-sdk': 'http://download.suse.de/ibs/SUSE/Products/SLE-SDK/12-SP2/x86_64/product/'
      'sle-sdk update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SDK/12-SP2/x86_64/update/'
      'SLE 12 update': 'http://download.suse.de/ibs/SUSE/Products/SLE-SERVER/12-SP2/x86_64/product/'
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES4/images/repo/SUSE-Enterprise-Storage-4-POOL-x86_64-Media1/'
      'Salt': 'http://download.opensuse.org/repositories/systemsmanagement:/saltstack:/products/SLE_12_SP2/'
      'ses4updates': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES4:/Update/standard/'
      'sle12updates': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update/standard/'
    packages:
      all:
        - vim
        - vim-data
        - python-setuptools
        - salt-minion
      admin:
        - salt-master
        - ntp
        - tree
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
      all:
        - systemctl restart salt-minion
        - systemctl enable salt-minion
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master
########################################
# SLE 12 SP1
########################################
SLE12-SP1:
  openattic:
    repos:
      #'Swiftgist': 'http://download.opensuse.org/repositories/home:/swiftgist:/monitoring/SLE_12/'
      #'OpenAttic': 'http://download.opensuse.org/repositories/home:/LenzGr:/openattic/SLE_12/'
      'OpenAttic': 'http://download.opensuse.org/repositories/filesystems:/openATTIC/SLE_12_SP1/'
      'sle-sdk': 'http://192.168.10.95/repo/products/sle-sdk/product'
      'SLE 12': 'http://192.168.10.95/repo/products/SLE_12/product'
      'Storage': 'http://192.168.10.95/repo/products/storage/product'
      'sle-sdk update': 'http://192.168.10.95/repo/updates/sle-sdk/update'
      'SLE 12 update': 'http://192.168.10.95/repo/updates/SLE_12/update'
      'Storage update': 'http://192.168.10.95/repo/updates/storage/update'
      #'System Monitoring': 'http://download.opensuse.org/repositories/server:/monitoring/SLE_12/'
    packages:
      admin:
        #- monitoring-plugins-http
        #- monitoring-plugins-swap
        #- monitoring-plugins-ssh
        #- monitoring-plugins-ping
        #- monitoring-plugins-disk
        - pnp4nagios-0.6.25-8.3
        - openattic
        - openattic-gui
      all:
        - vim
        - vim-data
        - man
    files:
      admin: true
    commands:
      admin:
        - ln -s /etc/icinga /etc/nagios3
        - ln -s /etc/icinga/icinga.cfg /etc/icinga/nagios.cfg
        - ln -s /usr/sbin/icinga /usr/sbin/nagios
        - systemctl enable apache2
        - systemctl start apache2
        - oaconfig install --allow-broken-hostname
  salt:
    repos:
      'sle-sdk': 'http://192.168.121.1/repo/products/sle-sdk-12-SP1/product'
      'sle-sdk update': 'http://192.168.121.1/repo/updates/sle-sdk-12-SP1/update'
      'SLE 12 SP1': 'http://192.168.121.1/repo/products/SLE_12-SP1/product'
      'SLE 12 SP1 update': 'http://192.168.121.1/repo/updates/SLE_12-SP1/update'
      #'SES-image': 'http://192.168.121.1/repo/products/ses-3.0-image/SUSE-Enterprise-Storage-3-POOL-x86_64-Build0122-Media1'
      'SES3.0': 'http://192.168.121.1/repo/products/ses-3.0/SUSE-Enterprise-Storage-3-POOL-x86_64-Build0135-Media1/'
      'SES3.0 Update': 'http://192.168.121.1/repo/updates/ses-3.0/standard'
      'systemsmanagement': 'http://192.168.121.1/repo/systemsmanagement:/saltstack:/SLE_12_SP1'
      'OpenAttic': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES4/standard/'
      #'OpenAttic': 'http://download.opensuse.org/repositories/home:/swiftgist/SLE_12_SP1/'
      'lrbd': 'http://download.suse.de/ibs/home:/swiftgist/SLE_12_SP1/'
    packages:
      all:
        - vim
        - vim-data
        - python-setuptools
        - salt-minion
          #- diamond
      admin:
        - ceph-deploy
        #- git
        - salt-master
        - ntp
          #- openattic
          #- openattic-gui
        - tree
      calamari:
        #- salt-master
        #- postgresql93-9.3.8-8.1
        #- calamari-server
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - zypper -n in -t pattern Basis-Devel
        - zypper -n in /tmp/deepsea-*.noarch.rpm
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph
        - chmod 1777 /tmp
        #- systemctl restart salt-minion
      all:
        - systemctl restart salt-minion
        - systemctl enable salt-minion
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master
########################################
# Configuration Section
########################################
#
# The default allows for reasonable exploration of Ceph installations and
# configurations.  Three monitors allow for a quorum and understanding failover,
# elections, etc.  Six data nodes with 30 osds and a pretend journal give
# enough flexibility to try replicas and erasure coding (e.g. k=3, m=2) while
# downing an osd or an entire node.
#
# This requires 5G RAM and ~32G disk space.  Initial vagrant up is ~28 minutes
# on a quad core.
default:
  description: "admin, 3 monitors, 6 data nodes with journals"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
      cluster: 172.16.2.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
      cluster: 172.16.2.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
    data5:
      management: 192.168.1.25
      public:  172.16.1.25
      cluster: 172.16.2.25
    data6:
      management: 192.168.1.26
      public:  172.16.1.26
      cluster: 172.16.2.26
  memory:
    admin: 1024
  disks:
    data1:
      hds: 5
      ssds: 1
    data2:
      hds: 5
      ssds: 1
    data3:
      hds: 5
      ssds: 1
    data4:
      hds: 5
      ssds: 1
    data5:
      hds: 5
      ssds: 1
    data6:
      hds: 5
      ssds: 1
# The small configuration is a typical demo setup of a monitor and three data
# nodes with two osds each.  This can be ideal for varifying that Vagrant and
# libvirt are behaving.  Simpler installations can be verified and the ceph
# command line can be explored.
#
# This requires 2.5G RAM and ~7G disk space.  Initial vagrant up is ~13 minutes
# on a quad core.
small:
  description: "admin, 1 monitor, 3 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.21.254
      cluster: 172.16.22.254
    mon1:
      management: 192.168.1.11
      public:  172.16.21.11
      cluster: 172.16.22.11
    data1:
      management: 192.168.1.21
      public:  172.16.21.21
      cluster: 172.16.22.21
    data2:
      management: 192.168.1.22
      public:  172.16.21.22
      cluster: 172.16.22.22
    data3:
      management: 192.168.1.23
      public:  172.16.21.23
      cluster: 172.16.22.23
  memory:
    admin: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
iscsi:
  description: "admin, 2 clients, 3 monitors, 5 data nodes, 3 gateways, 2 mds and calamari"
  nodes:
    admin:
      management: 192.168.11.254
      public:  172.16.11.254
      cluster: 172.16.12.254
    client1:
      management: 192.168.11.5
      public:  172.16.11.5
      cluster: 172.16.12.5
    client2:
      management: 192.168.11.6
      public:  172.16.11.6
      cluster: 172.16.12.6
    calamari:
      management: 192.168.11.10
      public:  172.16.11.10
      cluster: 172.16.12.10
    mon1:
      management: 192.168.11.11
      public:  172.16.11.11
      cluster: 172.16.12.11
    mon2:
      management: 192.168.11.12
      public:  172.16.11.12
      cluster: 172.16.12.12
    mon3:
      management: 192.168.11.13
      public:  172.16.11.13
      cluster: 172.16.12.13
    mds1:
      management: 192.168.11.14
      public:  172.16.11.14
      cluster: 172.16.12.14
    mds2:
      management: 192.168.11.15
      public:  172.16.11.15
      cluster: 172.16.12.15
    igw1:
      management: 192.168.11.16
      public:  172.16.11.16
      cluster: 172.16.12.16
    igw2:
      management: 192.168.11.17
      public:  172.16.11.17
      cluster: 172.16.12.17
    igw3:
      management: 192.168.11.18
      public:  172.16.11.18
      cluster: 172.16.12.18
    data1:
      management: 192.168.11.21
      public:  172.16.11.21
      cluster: 172.16.12.21
    data2:
      management: 192.168.11.22
      public:  172.16.11.22
      cluster: 172.16.12.22
    data3:
      management: 192.168.11.23
      public:  172.16.11.23
      cluster: 172.16.12.23
    data4:
      management: 192.168.11.24
      public:  172.16.11.24
      cluster: 172.16.12.24
    data5:
      management: 192.168.11.25
      public:  172.16.11.25
      cluster: 172.16.12.25
  cpu:
    admin: 4
  memory:
    admin: 3072
    calamari: 2048
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
fifteen:
  description: "admin, 2 clients, 3 monitors, 6 data nodes, 3 gateways"
  nodes:
    admin:
      management: 192.168.21.254
      public:  172.16.21.254
      cluster: 172.16.22.254
    client1:
      management: 192.168.21.5
      public:  172.16.21.5
      cluster: 172.16.22.5
    client2:
      management: 192.168.21.6
      public:  172.16.21.6
      cluster: 172.16.22.6
    mon1:
      management: 192.168.21.11
      public:  172.16.21.11
      cluster: 172.16.22.11
    mon2:
      management: 192.168.21.12
      public:  172.16.21.12
      cluster: 172.16.22.12
    mon3:
      management: 192.168.21.13
      public:  172.16.21.13
      cluster: 172.16.22.13
    igw1:
      management: 192.168.21.16
      public:  172.16.21.16
      cluster: 172.16.22.16
    igw2:
      management: 192.168.21.17
      public:  172.16.21.17
      cluster: 172.16.22.17
    igw3:
      management: 192.168.21.18
      public:  172.16.21.18
      cluster: 172.16.22.18
    data1:
      management: 192.168.21.21
      public:  172.16.21.21
      cluster: 172.16.22.21
    data2:
      management: 192.168.21.22
      public:  172.16.21.22
      cluster: 172.16.22.22
    data3:
      management: 192.168.21.23
      public:  172.16.21.23
      cluster: 172.16.22.23
    data4:
      management: 192.168.21.24
      public:  172.16.21.24
      cluster: 172.16.22.24
    data5:
      management: 192.168.21.25
      public:  172.16.21.25
      cluster: 172.16.22.25
    data6:
      management: 192.168.21.26
      public:  172.16.21.26
      cluster: 172.16.22.26
  cpu:
    admin: 4
  memory:
    admin: 3072
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
    data6:
      hds: 2
sixteen:
  description: "admin, 2 clients, 3 monitors, 5 data nodes, 3 gateways, 2 mds"
  nodes:
    admin:
      management: 192.168.31.254
      public:  172.16.31.254
      cluster: 172.16.32.254
    client1:
      management: 192.168.31.5
      public:  172.16.31.5
      cluster: 172.16.32.5
    client2:
      management: 192.168.31.6
      public:  172.16.31.6
      cluster: 172.16.32.6
    mon1:
      management: 192.168.31.11
      public:  172.16.31.11
      cluster: 172.16.32.11
    mon2:
      management: 192.168.31.12
      public:  172.16.31.12
      cluster: 172.16.32.12
    mon3:
      management: 192.168.31.13
      public:  172.16.31.13
      cluster: 172.16.32.13
    mds1:
      management: 192.168.31.14
      public:  172.16.31.14
      cluster: 172.16.32.14
    mds2:
      management: 192.168.31.15
      public:  172.16.31.15
      cluster: 172.16.32.15
    igw1:
      management: 192.168.31.16
      public:  172.16.31.16
      cluster: 172.16.32.16
    igw2:
      management: 192.168.31.17
      public:  172.16.31.17
      cluster: 172.16.32.17
    igw3:
      management: 192.168.31.18
      public:  172.16.31.18
      cluster: 172.16.32.18
    data1:
      management: 192.168.31.21
      public:  172.16.31.21
      cluster: 172.16.32.21
    data2:
      management: 192.168.31.22
      public:  172.16.31.22
      cluster: 172.16.32.22
    data3:
      management: 192.168.31.23
      public:  172.16.31.23
      cluster: 172.16.32.23
    data4:
      management: 192.168.31.24
      public:  172.16.31.24
      cluster: 172.16.32.24
    data5:
      management: 192.168.31.25
      public:  172.16.31.25
      cluster: 172.16.32.25
  cpu:
    admin: 4
  memory:
    admin: 3072
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
eighteen:
  description: "master, 2 clients, 3 monitors, 6 data nodes, 3 igw, 3 rgw"
  nodes:
    admin:
      management: 192.168.21.254
      public:  172.16.21.254
      cluster: 172.16.22.254
    client1:
      management: 192.168.21.5
      public:  172.16.21.5
      cluster: 172.16.22.5
    client2:
      management: 192.168.21.6
      public:  172.16.21.6
      cluster: 172.16.22.6
    rgw1:
      management: 192.168.21.8
      public:  172.16.21.8
      cluster: 172.16.22.8
    rgw2:
      management: 192.168.21.9
      public:  172.16.21.9
      cluster: 172.16.22.9
    rgw3:
      management: 192.168.21.10
      public:  172.16.21.10
      cluster: 172.16.22.10
    mon1:
      management: 192.168.21.11
      public:  172.16.21.11
      cluster: 172.16.22.11
    mon2:
      management: 192.168.21.12
      public:  172.16.21.12
      cluster: 172.16.22.12
    mon3:
      management: 192.168.21.13
      public:  172.16.21.13
      cluster: 172.16.22.13
    igw1:
      management: 192.168.21.16
      public:  172.16.21.16
      cluster: 172.16.22.16
    igw2:
      management: 192.168.21.17
      public:  172.16.21.17
      cluster: 172.16.22.17
    igw3:
      management: 192.168.21.18
      public:  172.16.21.18
      cluster: 172.16.22.18
    data1:
      management: 192.168.21.21
      public:  172.16.21.21
      cluster: 172.16.22.21
    data2:
      management: 192.168.21.22
      public:  172.16.21.22
      cluster: 172.16.22.22
    data3:
      management: 192.168.21.23
      public:  172.16.21.23
      cluster: 172.16.22.23
    data4:
      management: 192.168.21.24
      public:  172.16.21.24
      cluster: 172.16.22.24
    data5:
      management: 192.168.21.25
      public:  172.16.21.25
      cluster: 172.16.22.25
    data6:
      management: 192.168.21.26
      public:  172.16.21.26
      cluster: 172.16.22.26
  cpu:
    admin: 4
  memory:
    admin: 3072
    rgw1: 768
    rgw2: 768
    rgw3: 768
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
    data6: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
    data6:
      hds: 2
# The economical configuration represents a cold storage configuration that has
# no SSD journals.  Additionally, the monitors and admin node do not have an
# interface on the cluster network to save money on one less 10G interface.
#
# This requires 4G RAM and ~17G disk space.  Initial vagrant up is ~17 minutes
# on a quad core.
economical:
  description: "admin, 3 monitors, 4 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
  memory:
    admin: 1024
  disks:
    data1:
      hds: 4
    data2:
      hds: 4
    data3:
      hds: 4
    data4:
      hds: 4
# The workshop config.
# This requires 8G RAM and ~32G disk space.
workshop:
  description: "admin, 3 monitors, 6 data nodes with journals, 1 RGW node"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
      cluster: 172.16.2.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
      cluster: 172.16.2.13
    rgw:
      management: 192.168.1.14
      public: 172.16.1.14
      cluster: 172.16.2.14
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
    data5:
      management: 192.168.1.25
      public:  172.16.1.25
      cluster: 172.16.2.25
    data6:
      management: 192.168.1.26
      public:  172.16.1.26
      cluster: 172.16.2.26
  memory:
    admin: 1024
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
    data6: 1024
    rgw: 1024
  disks:
    data1:
      hds: 5
      ssds: 1
    data2:
      hds: 5
      ssds: 1
    data3:
      hds: 5
      ssds: 1
    data4:
      hds: 5
      ssds: 1
    data5:
      hds: 5
      ssds: 1
    data6:
      hds: 5
      ssds: 1
